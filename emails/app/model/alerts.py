import logging
import httplib, urllib, urllib2
import re
import htmlentitydefs
from BeautifulSoup import BeautifulSoup
from webapp.blurb.models import Blurb

GOOGLE_URL = "www.google.com"
ALERTS_URL = "/alerts/create?hl=en&gl=us"
TYPES_NAME = "t"
EMAIL_NAME = "e"
FREQUENCY_NAME = "f"
LENGTH_NAME = "l"
TERM_NAME = "q"

TYPES = {
    'comprehensive' : '7',
    'news' : '1',
    'web' : '2',
    'blogs' : '4',
    'groups' : '8',
    'video' : '9'
}

FREQUENCY = {
    'instant' : '0',
    'day' : '1',
    'week' : '6'
}

LOG = logging.getLogger("Parse Alerts")

remove = re.compile('(?:(/alerts/remove[^">]*)">)|(/alerts/remove.*\w)')
verify = re.compile("/alerts/verify.*\w")
byline = re.compile("By ([a-zA-Z]+ [a-zA-Z]+) ")

def create_alert(term, email, type='comprehensive', frequency='instant', length=50):

    """
    Creates a google alert for the given term and
    email.
    """

    pdict = create_params(term, email, type, frequency, length)
    params =  urllib.urlencode(pdict)
    headers = {'Content-type': 'application/x-www-form-urlencoded; charset=utf-8'}
    conn = httplib.HTTPConnection(GOOGLE_URL)
    conn.request("POST", ALERTS_URL, params, headers)
    response = conn.getresponse()
    assert response.status == 200, "Something went wrong, your alert was not created."
    conn.close()


def create_params(term, email, typekey, freqkey, length):

    """
    Creates a url encoded POST string to send to
    google alerts service.
    """

    assert freqkey in FREQUENCY.keys(), "Must be one of %s" % FREQUENCY.keys()
    assert typekey in TYPES.keys(), "Must be one of %s" % TYPES.keys()
    assert length == 50 or length == 20, "Length must be 50 or 20."

    type = TYPES[typekey]
    frequency = FREQUENCY[freqkey]
    return {
        TERM_NAME : term, 
        EMAIL_NAME : email,
        TYPES_NAME : type, 
        FREQUENCY_NAME : frequency,
        LENGTH_NAME : length}


def confirm_alert(msg):

    """
    Takes a lamson message object, finds the confirmation
    url and confirms the creation of the alert.
    """
    
    url = get_conf_url(msg.base.body)
    status, body = send_confirmation(url)
    assert status == 200
    assert confirmed(body)


def send_confirmation(url):
    """
    Generates a request to the given
    confirmation url.
    """
    conn = httplib.HTTPConnection(GOOGLE_URL)
    conn.request("GET", url)
    response = conn.getresponse()
    body = response.read()
    status = response.status
    conn.close()
    return status, body



def get_conf_url(body):
    """
    Scrapes the confirmation url from
    the confirmation email.
    """
    url = verify.findall(body)[0]
    return url


def confirmed(html):
    """
    Takes the html generated by the confirmation request and
    tries to determine if it was successfully confirmed or not.
    """
    soup = BeautifulSoup(html)

    error = soup.find(text=re.compile("error"))
    normal = soup.find(text=re.compile("Verified"))

    if error:
        message = error.replace("\n", "")
    elif normal:
        message = normal.replace("\n", "")
    else:
        message = ""

    if message == "An error has occurred.":
        return False
    elif message == "Google Alert Verified":
        return True
    else:
        return False


def get_html_stubs(html):
    """
    Takes in the html from an alerts email
    and returns a list.
    """
    soup = BeautifulSoup(html)
    # the first two rows are just general information about the alerts
    trs = soup.body.find('div', recursive=False).findAll('table', recursive=False)[0].findAll('tr', recursive=False)[2:]
    
    stubs = []
    for tr in trs:
        if tr.td.find('table', recursive=False):
            stubs.append(tr.td.table.td)
        else:
            stubs.append(tr.td)
    return stubs


def get_remove_url(html):
    """
    Return the url used for removing this alert.
    """
    left, right = remove.findall(html)[0]
    answer = left or right
    return left or right


def disable_alert(url):
    """
    Takes a url to call in order to disable an
    alert.
    """
    assert url.startswith("/alerts"), "Url of the alert to disable is not valid: %s." % url
    conn = httplib.HTTPConnection(GOOGLE_URL)
    conn.request("GET", url)
    response = conn.getresponse()
    assert response.status == 200, "Disable alert failed. Google alerts status code was %s" % response.status
    conn.close()


def get_raw_alert(stub):
    """
    Given a stub of html beautiful soup, return
    a dictionary representing the alert.
    """

    #the first font tag contains the text nodes we want.
    blurb = ''.join(stub.find('font', recursive=False).findAll(text=True)).replace("\n", "")
    title = ''.join(stub.find('a', recursive=False).findAll(text=True)).replace("\n", "")
    source = ""


    #sometimes there is not a source
    font = stub.find('font', recursive=False).font
    if font:
        source = font.find(text=True)


    # remove the source from the beginning of the blurb if it is there.
    try:
        index = blurb.index(source)
        if index == 0:
            hasSource = True
        else:
            hasSource = False
    except ValueError:
        hasSource = False

    if hasSource:
        blurb = blurb[len(source):]

    #Google wraps up the direct link in a query string, which goes
    #to them first then redirects. This gets the big link then pulls
    # the actual link out of the query string.
    bigUrl = stub.find('a', recursive=False)['href']
    url = urllib2.unquote(urllib2.urlparse.parse_qs(bigUrl)['q'][0])

    #get the byline
    by = ""
    mtch = byline.match(blurb)
    if mtch:
        by = mtch.groups()[0]
        
    raw = {'blurb' : blurb,
            'title' : title,
            'source' : str(source),
            'byline' : by,
            'url' : url}

    for key in raw.keys():
        raw[key] = __unescape(raw[key])
    return raw

def create_blurbs(msg, alert):
    """
    Given a lamson message and an alert, create
    the blurb objects and persist to the database.
    """

    stubs = get_html_stubs(msg.body())

    rawAlerts = []
    for stub in stubs:
        try:
            raw = get_raw_alert(stub)
            rawAlerts.append(raw)
        except Exception as e:
            LOG.debug("Failed to parse this stub:\n%s" % str(stub))


    blurbs = []
    for raw in rawAlerts:

        blurb = Blurb(alert=alert,
                      client=alert.client,
                      byline=raw['byline'],
                      source=raw['source'],
                      title=raw['title'],
                      blurb=raw['blurb'],
                      url=raw['url'])
        blurb.save()
        blurbs.append(blurb)

    return blurbs




##
# Removes HTML or XML character references and entities from a text string.
#
# @param text The HTML (or XML) source text.
# @return The plain text, as a Unicode string, if necessary.
def __unescape(text):
    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            try:
                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])
            except KeyError:
                pass
        return text # leave as is
    return re.sub("&#?\w+;", fixup, text)



